{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "executionInfo": {
     "elapsed": 857,
     "status": "ok",
     "timestamp": 1697443265451,
     "user": {
      "displayName": "Mr. Ben",
      "userId": "14464510081174440893"
     },
     "user_tz": -120
    },
    "id": "2hNN7WALjFYW",
    "outputId": "c4c7fcee-ca88-453b-fb35-3925a74950cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Daten...\n",
      "Verfügbare Spalten: ['Document Title', 'Authors', 'Author Affiliations', 'Publication Title', 'Date Added To Xplore', 'Publication Year', 'Volume', 'Issue', 'Start Page', 'End Page', 'Abstract', 'ISSN', 'ISBNs', 'DOI', 'Funding Information', 'PDF Link', 'Author Keywords', 'IEEE Terms', 'Mesh_Terms', 'Article Citation Count', 'Patent Citation Count', 'Reference Count', 'License', 'Online Date', 'Issue Date', 'Meeting Date', 'Publisher', 'Document Identifier', 'is_ai_related']\n",
      "\n",
      "Datensatz Größe: 642\n",
      "\n",
      "Anzahl der Abstracts: 642\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Deaktiviere Altair Row Limit\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Daten laden\n",
    "print(\"Lade Daten...\")\n",
    "df = pd.read_csv('AI_Related_Papers_Cleaned.csv')\n",
    "print(\"Verfügbare Spalten:\", df.columns.tolist())\n",
    "print(\"\\nDatensatz Größe:\", len(df))\n",
    "\n",
    "# Extrahiere Dokumente für Topic Modeling\n",
    "docs = df[\"Abstract\"].to_list()\n",
    "print(\"\\nAnzahl der Abstracts:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1697443367986,
     "user": {
      "displayName": "Mr. Ben",
      "userId": "14464510081174440893"
     },
     "user_tz": -120
    },
    "id": "ErdCJCWekPLD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysiere Publikationstrends...\n",
      "Publikationstrend-Visualisierung gespeichert als 'publication_trends.html'\n"
     ]
    }
   ],
   "source": [
    "# Publikationstrend-Analyse\n",
    "print(\"\\nAnalysiere Publikationstrends...\")\n",
    "yearly_pubs = df.groupby('Publication Year').size().reset_index(name='count')\n",
    "\n",
    "# Erstelle ein vollständiges DataFrame für alle Jahre von 1994 bis 2024\n",
    "all_years = pd.DataFrame({'Publication Year': range(1994, 2025)})\n",
    "yearly_pubs = pd.merge(all_years, yearly_pubs, on='Publication Year', how='left')\n",
    "yearly_pubs['count'] = yearly_pubs['count'].fillna(0)\n",
    "\n",
    "# Erstelle den Basis-Linienplot\n",
    "base = alt.Chart(yearly_pubs).encode(\n",
    "    x=alt.X('Publication Year:Q',\n",
    "            scale=alt.Scale(domain=[1994, 2024]),\n",
    "            axis=alt.Axis(\n",
    "                tickCount=yearly_pubs['Publication Year'].nunique(),\n",
    "                format='d'\n",
    "            ),\n",
    "            title='Jahr'),\n",
    "    y=alt.Y('count:Q',\n",
    "            title='Anzahl Publikationen')\n",
    ")\n",
    "\n",
    "# Kombiniere alle Elemente für Publikationstrend\n",
    "pub_trend_chart = (base.mark_line() + base.mark_point(size=50) + \n",
    "    base.mark_text(\n",
    "        align='center',\n",
    "        baseline='bottom',\n",
    "        dy=-10\n",
    "    ).encode(\n",
    "        text=alt.Text('count:Q', format='.0f')\n",
    "    )).properties(\n",
    "    title='Entwicklung der KI-Publikationen im Software Engineering',\n",
    "    width=1000,\n",
    "    height=500\n",
    ").configure_axis(\n",
    "    grid=True,\n",
    "    gridOpacity=0.3\n",
    ").configure_view(\n",
    "    stroke=None\n",
    ")\n",
    "\n",
    "pub_trend_chart.save('publication_trends.html')\n",
    "print(\"Publikationstrend-Visualisierung gespeichert als 'publication_trends.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1697443395788,
     "user": {
      "displayName": "Mr. Ben",
      "userId": "14464510081174440893"
     },
     "user_tz": -120
    },
    "id": "xjXhaxSejFYe",
    "outputId": "0ef74aaa-1c2e-455b-bee9-2e2a4dc6eb56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Führe Topic Modeling durch...\n",
      "\n",
      "Gefundene Topics:\n",
      "    Topic  Count                                               Name  \\\n",
      "0      -1    171                -1_software_faults_developers_tools   \n",
      "1       0    198                    0_code_program_software_patches   \n",
      "2       1     85  1_adversarial_vulnerabilities_vulnerability_vu...   \n",
      "3       2     65  2_maintainability_software_reliability_mainten...   \n",
      "4       3     29                     3_testing_coverage_test_faults   \n",
      "5       4     26      4_metamodels_metamodel_modeling_specification   \n",
      "6       5     23                             5_apps_android_app_gui   \n",
      "7       6     22       6_concurrency_scheduling_protocols_deadlocks   \n",
      "8       7      9        7_markovian_markov_stochastic_probabilistic   \n",
      "9       8      8  8_programming_programmers_developers_collabora...   \n",
      "10      9      6                        9_icse_conference_2022_ieee   \n",
      "\n",
      "                                       Representation  \\\n",
      "0   [software, faults, developers, tools, programs...   \n",
      "1   [code, program, software, patches, automated, ...   \n",
      "2   [adversarial, vulnerabilities, vulnerability, ...   \n",
      "3   [maintainability, software, reliability, maint...   \n",
      "4   [testing, coverage, test, faults, mutants, mut...   \n",
      "5   [metamodels, metamodel, modeling, specificatio...   \n",
      "6   [apps, android, app, gui, guis, mobile, runtim...   \n",
      "7   [concurrency, scheduling, protocols, deadlocks...   \n",
      "8   [markovian, markov, stochastic, probabilistic,...   \n",
      "9   [programming, programmers, developers, collabo...   \n",
      "10  [icse, conference, 2022, ieee, 2023, acm, 2024...   \n",
      "\n",
      "                                  Representative_Docs  \n",
      "0   [On-board embedded software developed for spac...  \n",
      "1   [Automated program repair is the problem of au...  \n",
      "2   [Deep learning (DL) plays a more and more impo...  \n",
      "3   [In software reliability modeling, the paramet...  \n",
      "4   [The test case generation is intrinsically a m...  \n",
      "5   [Formal methods and supporting tools have a lo...  \n",
      "6   [Web applications are widely adopted and their...  \n",
      "7   [In this paper, we describe Teapot, a domain-s...  \n",
      "8   [Equivalence relations can be used to reduce t...  \n",
      "9   [Pair Programming is one of the most studied a...  \n",
      "10  [What's new with ICSE 2022 ICSE 2022 is here! ...  \n",
      "\n",
      "Erstelle erweiterte Topic-Visualisierungen...\n"
     ]
    }
   ],
   "source": [
    "# Topic Modeling\n",
    "print(\"\\nFühre Topic Modeling durch...\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=10,\n",
    "    n_components=2,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=5,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=CountVectorizer(stop_words=[]),\n",
    "    ctfidf_model=ClassTfidfTransformer(),\n",
    "    representation_model=KeyBERTInspired()\n",
    ")\n",
    "\n",
    "# Fit Model und reduziere Topics\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "topic_model.reduce_topics(docs, nr_topics=11)\n",
    "\n",
    "# Topic Info ausgeben\n",
    "print(\"\\nGefundene Topics:\")\n",
    "print(topic_model.get_topic_info())\n",
    "\n",
    "# Erweiterte Topic-Visualisierungen\n",
    "print(\"\\nErstelle erweiterte Topic-Visualisierungen...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1697443395789,
     "user": {
      "displayName": "Mr. Ben",
      "userId": "14464510081174440893"
     },
     "user_tz": -120
    },
    "id": "cfTcUTyxjFYe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokument-Visualisierung gespeichert als 'topic_visualization.html'\n"
     ]
    }
   ],
   "source": [
    "# Document Visualization\n",
    "fig_docs = topic_model.visualize_documents(\n",
    "    docs,\n",
    "    topics=list(range(-1, len(topic_model.get_topics())-1)),\n",
    "    width=1200,\n",
    "    height=1000\n",
    ")\n",
    "fig_docs.write_html(\"topic_visualization.html\")\n",
    "print(\"Dokument-Visualisierung gespeichert als 'topic_visualization.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Themen-Ähnlichkeiten\n",
    "topic_similarities = topic_model.visualize_topics(width=1000, height=1000)\n",
    "topic_similarities.write_html(\"topic_similarities.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1697443406249,
     "user": {
      "displayName": "Mr. Ben",
      "userId": "14464510081174440893"
     },
     "user_tz": -120
    },
    "id": "B3jcLyo1ivFl",
    "outputId": "bb3cd857-6cfb-49ce-b42f-331a9f481075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeitliche Entwicklung gespeichert als 'topics_over_time.html'\n"
     ]
    }
   ],
   "source": [
    "# Zeitliche Entwicklung der Topics\n",
    "timestamps = df[\"Publication Year\"].tolist()\n",
    "topics_over_time = topic_model.topics_over_time(docs, timestamps)\n",
    "fig_time = topic_model.visualize_topics_over_time(\n",
    "    topics_over_time,\n",
    "    width=1000\n",
    ")\n",
    "fig_time.write_html(\"topics_over_time.html\")\n",
    "print(\"Zeitliche Entwicklung gespeichert als 'topics_over_time.html'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Netzwerk erstellen\n",
    "G = nx.Graph()\n",
    "\n",
    "# Technologie-Kategorien definieren\n",
    "technology_categories = {\n",
    "    'Machine Learning': ['ml', 'machine learning', 'classification'],\n",
    "    'Deep Learning': ['neural network', 'deep learning', 'cnn', 'rnn'],\n",
    "    'Testing & Quality': ['testing', 'quality', 'test generation'],\n",
    "    'Code Analysis': ['code analysis', 'program analysis', 'static analysis'],\n",
    "    'Requirements': ['requirements', 'specification', 'validation'],\n",
    "    'Maintenance': ['maintenance', 'refactoring', 'evolution']\n",
    "}\n",
    "\n",
    "# Verbindungen analysieren\n",
    "connections = []\n",
    "for abstract in df['Abstract']:\n",
    "    if isinstance(abstract, str):\n",
    "        abstract = abstract.lower()\n",
    "        found_categories = []\n",
    "        for category, keywords in technology_categories.items():\n",
    "            if any(keyword in abstract for keyword in keywords):\n",
    "                found_categories.append(category)\n",
    "        for i in range(len(found_categories)):\n",
    "            for j in range(i+1, len(found_categories)):\n",
    "                connections.append((found_categories[i], found_categories[j]))\n",
    "\n",
    "connection_counts = Counter(connections)\n",
    "\n",
    "# Knoten und Kanten hinzufügen\n",
    "for category in technology_categories:\n",
    "    G.add_node(category)\n",
    "\n",
    "for (cat1, cat2), weight in connection_counts.items():\n",
    "    G.add_edge(cat1, cat2, weight=weight)\n",
    "\n",
    "# Metriken berechnen\n",
    "degrees = dict(G.degree(weight='weight'))\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight')\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "\n",
    "# Layout mit mehr Platz erstellen\n",
    "pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "\n",
    "# Edge Trace erstellen\n",
    "edge_x = []\n",
    "edge_y = []\n",
    "edge_text = []\n",
    "\n",
    "for edge in G.edges(data=True):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "    text = f\"{edge[2]['weight']}\"\n",
    "    edge_text.extend([text, text, None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=1, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines+text',\n",
    "    text=edge_text,\n",
    "    textposition='middle center',\n",
    "    textfont=dict(size=8)\n",
    ")\n",
    "\n",
    "# Node Trace erstellen\n",
    "node_x = []\n",
    "node_y = []\n",
    "node_text = []\n",
    "\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    label = (f\"{node}<br>\" +\n",
    "             f\"Verbindungen: {degrees[node]}<br>\" +\n",
    "             f\"Zentralität: {eigenvector_centrality[node]:.2f}<br>\" +\n",
    "             f\"Vermittlerrolle: {betweenness_centrality[node]:.2f}\")\n",
    "    node_text.append(label)\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers+text',\n",
    "    hoverinfo='none',\n",
    "    text=node_text,\n",
    "    textposition=\"top center\",\n",
    "    textfont=dict(size=6.5, color='black'),\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        size=[40 + (degrees[node]/max(degrees.values())) * 30 for node in G.nodes()],\n",
    "        color=[eigenvector_centrality[node] for node in G.nodes()],\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(\n",
    "            title='Zentralität',\n",
    "            thickness=15,\n",
    "            len=0.7,\n",
    "            xanchor='left',    # Anker links setzen\n",
    "            x=1.02,            # Nach rechts außen verschieben\n",
    "            yanchor='middle',  # Vertikal zentrieren\n",
    "            y=0.5              # Mittige Position\n",
    "        ),\n",
    "        line_width=2,\n",
    "        line=dict(color='white')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Figure mit optimiertem Layout erstellen\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                layout=go.Layout(\n",
    "                    title=dict(\n",
    "                        text='Netzwerk der KI-Technologien im Software Engineering<br>' +\n",
    "                             'Knotengröße = Anzahl der Verbindungen<br>' +\n",
    "                             'Knotenfarbe = Zentralität im Netzwerk<br>' +\n",
    "                             'Kantenbeschriftung = Anzahl gemeinsamer Erwähnungen',\n",
    "                        y=0.95,\n",
    "                        x=0.5,\n",
    "                        xanchor='center',\n",
    "                        yanchor='top',\n",
    "                        font=dict(size=10)\n",
    "                    ),\n",
    "                    showlegend=False,\n",
    "                    # Mehr Platz auf der rechten Seite für die Farbskala\n",
    "                    margin=dict(b=20, l=5, r=100, t=90),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    plot_bgcolor='white',\n",
    "                    # Anpassung der Plot-Größe um Platz für die Farbskala zu schaffen\n",
    "                    width=1000,   # Gesamtbreite\n",
    "                    height=800,   # Gesamthöhe\n",
    "                    annotations=[\n",
    "                        dict(\n",
    "                            text=f\"Stärkste Verbindung: {max(dict(G.edges).items(), key=lambda x: x[1]['weight'])[1]['weight']} Erwähnungen<br>\" +\n",
    "                                 f\"Zentrale Technologie: {max(eigenvector_centrality.items(), key=lambda x: x[1])[0]}\",\n",
    "                            showarrow=False,\n",
    "                            xref='paper', \n",
    "                            yref='paper',\n",
    "                            x=0.01,\n",
    "                            y=-0.1,\n",
    "                            font=dict(size=12)\n",
    "                        )\n",
    "                    ]\n",
    "                ))\n",
    "\n",
    "# Speichern der Visualisierung\n",
    "fig.write_html(\"network_analysis_adjusted.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1697443406250,
     "user": {
      "displayName": "Mr. Ben",
      "userId": "14464510081174440893"
     },
     "user_tz": -120
    },
    "id": "0_VzqpwCN4gM",
    "outputId": "f0c29fb0-4af9-47f6-cab6-5ac119bfcc9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysiere Technologie-Trends...\n",
      "Technologie-Trend-Visualisierung gespeichert als 'technology_trends.html'\n"
     ]
    }
   ],
   "source": [
    "# Technologie-Trend-Analyse\n",
    "print(\"\\nAnalysiere Technologie-Trends...\")\n",
    "tech_keywords = tech_categories = {\n",
    "    'Machine Learning': ['machine learning', 'ml', 'supervised learning', 'unsupervised learning'],\n",
    "    'Deep Learning': ['deep learning', 'neural network', 'cnn', 'rnn', 'lstm'],\n",
    "    'Natural Language Processing': ['nlp', 'natural language processing', 'text analysis', 'language model'],\n",
    "    'Computer Vision': ['computer vision', 'image processing', 'object detection'],\n",
    "    'Reinforcement Learning': ['reinforcement learning', 'rl', 'q-learning'],\n",
    "    'Expert Systems': ['expert system', 'knowledge base', 'rule-based']\n",
    "}\n",
    "\n",
    "tech_trends = pd.DataFrame()\n",
    "for keyword in tech_keywords:\n",
    "    tech_trends[keyword] = df['Abstract'].str.contains(keyword, case=False).astype(int)\n",
    "\n",
    "# Aggregiere nach Jahr\n",
    "yearly_tech_trends = pd.concat([\n",
    "    df['Publication Year'],\n",
    "    tech_trends\n",
    "], axis=1).groupby('Publication Year').sum().reset_index()\n",
    "\n",
    "# Erstelle ein vollständiges DataFrame für alle Jahre von 1994 bis 2024\n",
    "all_years = pd.DataFrame({'Publication Year': range(1994, 2025)})\n",
    "yearly_tech_trends = pd.merge(all_years, yearly_tech_trends, on='Publication Year', how='left')\n",
    "yearly_tech_trends = yearly_tech_trends.fillna(0)\n",
    "\n",
    "# Erstelle eine Altair-Visualisierung\n",
    "tech_trend_chart = alt.Chart(\n",
    "    yearly_tech_trends.melt('Publication Year', var_name='Technology', value_name='Count')\n",
    ").mark_line(point=True).encode(\n",
    "    x=alt.X('Publication Year:Q',\n",
    "            scale=alt.Scale(domain=[1994, 2024]),\n",
    "            axis=alt.Axis(\n",
    "                tickCount=yearly_tech_trends['Publication Year'].nunique(),\n",
    "                format='d'  # 'd' format für ganze Zahlen\n",
    "            ),\n",
    "            title='Jahr'),\n",
    "    y=alt.Y('Count:Q', title='Anzahl der Erwähnungen'),\n",
    "    color=alt.Color('Technology:N', title='Technologie'),\n",
    "    tooltip=['Publication Year:Q', 'Technology:N', 'Count:Q']\n",
    ").properties(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    title='Entwicklung verschiedener KI-Technologien über die Zeit'\n",
    ")\n",
    "\n",
    "# Speichere die Visualisierung als HTML\n",
    "tech_trend_chart.save('technology_trends.html')\n",
    "print(\"Technologie-Trend-Visualisierung gespeichert als 'technology_trends.html'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Führe Zitationsanalyse durch...\n",
      "\n",
      "Top 10 meist-zitierte Papers:\n",
      "\n",
      "Titel: Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings\n",
      "Zitationen: 849.0\n",
      "Jahr: 2008\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: GenProg: A Generic Method for Automatic Software Repair\n",
      "Zitationen: 718.0\n",
      "Jahr: 2012\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: A critique of software defect prediction models\n",
      "Zitationen: 694.0\n",
      "Jahr: 1999\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: Simplifying and isolating failure-inducing input\n",
      "Zitationen: 670.0\n",
      "Jahr: 2002\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: Model-checking algorithms for continuous-time Markov chains\n",
      "Zitationen: 526.0\n",
      "Jahr: 2003\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: Change Distilling:Tree Differencing for Fine-Grained Source Code Change Extraction\n",
      "Zitationen: 452.0\n",
      "Jahr: 2007\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: Quantitative analysis of faults and failures in a complex software system\n",
      "Zitationen: 442.0\n",
      "Jahr: 2000\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks\n",
      "Zitationen: 418.0\n",
      "Jahr: 2006\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: Machine Learning Testing: Survey, Landscapes and Horizons\n",
      "Zitationen: 417.0\n",
      "Jahr: 2022\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Titel: An Empirical Comparison of Model Validation Techniques for Defect Prediction Models\n",
      "Zitationen: 377.0\n",
      "Jahr: 2017\n",
      "Journal: IEEE Transactions on Software Engineering\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Zitationsanalyse\n",
    "print(\"\\nFühre Zitationsanalyse durch...\")\n",
    "\n",
    "# Top zitierte Papers\n",
    "print(\"\\nTop 10 meist-zitierte Papers:\")\n",
    "top_cited = df.nlargest(10, 'Article Citation Count')\n",
    "for idx, paper in top_cited.iterrows():\n",
    "    print(f\"\\nTitel: {paper['Document Title']}\")\n",
    "    print(f\"Zitationen: {paper['Article Citation Count']}\")\n",
    "    print(f\"Jahr: {paper['Publication Year']}\")\n",
    "    print(f\"Journal: {paper['Publication Title']}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# Zitationsentwicklung über Zeit\n",
    "yearly_citations = df.groupby('Publication Year')['Article Citation Count'].sum().reset_index()\n",
    "\n",
    "# Visualisierung mit Altair\n",
    "citation_chart = alt.Chart(yearly_citations).mark_line(point=True).encode(\n",
    "    x=alt.X('Publication Year:Q',\n",
    "            scale=alt.Scale(domain=[1994, 2024]),\n",
    "            axis=alt.Axis(format='d'),\n",
    "            title='Jahr'),\n",
    "    y=alt.Y('Article Citation Count:Q',\n",
    "            title='Gesamtzahl der Zitationen'),\n",
    "    tooltip=['Publication Year:Q', 'Article Citation Count:Q']\n",
    ").properties(\n",
    "    title='Entwicklung der Zitationen über Zeit',\n",
    "    width=1000,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "# Füge Textlabels hinzu\n",
    "text = alt.Chart(yearly_citations).mark_text(\n",
    "    align='center',\n",
    "    baseline='bottom',\n",
    "    dy=-10\n",
    ").encode(\n",
    "    x='Publication Year:Q',\n",
    "    y='Article Citation Count:Q',\n",
    "    text=alt.Text('Article Citation Count:Q', format='.0f')\n",
    ")\n",
    "\n",
    "# Kombiniere Chart und Labels\n",
    "final_citation_chart = (citation_chart + text).configure_axis(\n",
    "    grid=True,\n",
    "    gridOpacity=0.3\n",
    ")\n",
    "final_citation_chart.save('citation_trends.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysiere führende Journals...\n",
      "\n",
      "Journal-Analyse abgeschlossen und gespeichert\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal</th>\n",
       "      <th>Anzahl Papers</th>\n",
       "      <th>Gesamtzitationen</th>\n",
       "      <th>Durchschnittliche Zitationen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IEEE Transactions on Software Engineering</td>\n",
       "      <td>409</td>\n",
       "      <td>22257.0</td>\n",
       "      <td>54.418093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012 34th International Conference on Software...</td>\n",
       "      <td>31</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>33.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022 IEEE/ACM 44th International Conference on...</td>\n",
       "      <td>62</td>\n",
       "      <td>725.0</td>\n",
       "      <td>11.693548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023 IEEE/ACM 45th International Conference on...</td>\n",
       "      <td>61</td>\n",
       "      <td>620.0</td>\n",
       "      <td>10.163934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024 IEEE/ACM 46th International Conference on...</td>\n",
       "      <td>79</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.848101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Journal  Anzahl Papers  \\\n",
       "4          IEEE Transactions on Software Engineering            409   \n",
       "0  2012 34th International Conference on Software...             31   \n",
       "1  2022 IEEE/ACM 44th International Conference on...             62   \n",
       "2  2023 IEEE/ACM 45th International Conference on...             61   \n",
       "3  2024 IEEE/ACM 46th International Conference on...             79   \n",
       "\n",
       "   Gesamtzitationen  Durchschnittliche Zitationen  \n",
       "4           22257.0                     54.418093  \n",
       "0            1042.0                     33.612903  \n",
       "1             725.0                     11.693548  \n",
       "2             620.0                     10.163934  \n",
       "3              67.0                      0.848101  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Journal Analyse\n",
    "print(\"\\nAnalysiere führende Journals...\")\n",
    "journal_stats = df.groupby('Publication Title').agg({\n",
    "    'Document Title': 'count',\n",
    "    'Article Citation Count': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "journal_stats.columns = ['Journal', 'Anzahl Papers', 'Gesamtzitationen']\n",
    "journal_stats['Durchschnittliche Zitationen'] = journal_stats['Gesamtzitationen'] / journal_stats['Anzahl Papers']\n",
    "journal_stats = journal_stats.sort_values('Gesamtzitationen', ascending=False)\n",
    "\n",
    "# Top 10 Journals visualisieren\n",
    "top_journals = journal_stats #.head(10)\n",
    "journal_chart = alt.Chart(top_journals).mark_bar().encode(\n",
    "    y=alt.Y('Journal:N', \n",
    "            sort='-x',\n",
    "            title='Journal'),\n",
    "    x=alt.X('Gesamtzitationen:Q',\n",
    "            title='Gesamtzahl der Zitationen'),\n",
    "    tooltip=['Journal', 'Anzahl Papers', 'Gesamtzitationen', 'Durchschnittliche Zitationen']\n",
    ").properties(\n",
    "    title='Top 5 Journals nach Gesamtzitationen',\n",
    "    width=1000,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "# Füge Werte an den Balken hinzu\n",
    "text = alt.Chart(top_journals).mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=3\n",
    ").encode(\n",
    "    y='Journal:N',\n",
    "    x='Gesamtzitationen:Q',\n",
    "    text=alt.Text('Gesamtzitationen:Q', format='.0f')\n",
    ")\n",
    "\n",
    "final_journal_chart = (journal_chart + text).configure_axis(\n",
    "    grid=True,\n",
    "    gridOpacity=0.3\n",
    ")\n",
    "final_journal_chart.save('journal_analysis.html')\n",
    "\n",
    "print(\"\\nJournal-Analyse abgeschlossen und gespeichert\")\n",
    "top_journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysiere Länder und Institutionen...\n",
      "\n",
      "Top 10 Länder und ihre Publikationszahlen:\n",
      "USA: 193\n",
      "China: 140\n",
      "UK: 48\n",
      "Canada: 44\n",
      "Italy: 32\n",
      "Germany: 29\n",
      "Australia: 20\n",
      "Norway: 11\n",
      "Switzerland: 10\n",
      "Spain: 9\n"
     ]
    }
   ],
   "source": [
    "# Länder- und Institutionsanalyse\n",
    "print(\"\\nAnalysiere Länder und Institutionen...\")\n",
    "\n",
    "def extract_country(affiliation):\n",
    "    # Spezifische Länder und ihre Varianten\n",
    "    common_countries = {\n",
    "        'USA': ['USA', 'United States', 'U.S.A', 'United States of America', 'United States of'],\n",
    "        'UK': ['UK', 'United Kingdom', 'England', 'Scotland', 'Wales', 'Britain'],\n",
    "        'Germany': ['Germany', 'Deutschland'],\n",
    "        'China': ['China', 'P.R. China', 'People\\'s Republic of China'],\n",
    "        'Canada': ['Canada'],\n",
    "        'Japan': ['Japan'],\n",
    "        'India': ['India'],\n",
    "        'Australia': ['Australia'],\n",
    "        'France': ['France'],\n",
    "        'Italy': ['Italy'],\n",
    "        'Spain': ['Spain'],\n",
    "        'Netherlands': ['Netherlands', 'The Netherlands'],\n",
    "        'Switzerland': ['Switzerland'],\n",
    "        'South Korea': ['South Korea', 'Korea'],\n",
    "        'Singapore': ['Singapore'],\n",
    "        'Brazil': ['Brazil'],\n",
    "        'Sweden': ['Sweden'],\n",
    "        'Norway': ['Norway'],\n",
    "        'Denmark': ['Denmark'],\n",
    "        'Austria': ['Austria']\n",
    "    }\n",
    "    \n",
    "    if pd.isna(affiliation):\n",
    "        return None\n",
    "        \n",
    "    affiliation = str(affiliation).lower()\n",
    "    for country, variants in common_countries.items():\n",
    "        if any(variant.lower() in affiliation for variant in variants):\n",
    "            return country\n",
    "    return None\n",
    "\n",
    "def extract_institution(affiliation):\n",
    "    if pd.isna(affiliation):\n",
    "        return None\n",
    "    \n",
    "    # Erste Institution nehmen (falls mehrere angegeben sind)\n",
    "    institution = affiliation.split(';')[0].strip()\n",
    "    \n",
    "    # Kurze oder leere Institutionsnamen ausfiltern\n",
    "    if len(institution) < 5:\n",
    "        return None\n",
    "        \n",
    "    return institution\n",
    "\n",
    "# Länder und Institutionen extrahieren\n",
    "df['Country'] = df['Author Affiliations'].apply(extract_country)\n",
    "df['Institution'] = df['Author Affiliations'].apply(extract_institution)\n",
    "\n",
    "# Länderanalyse\n",
    "valid_countries = df['Country'].dropna().value_counts().head(10)\n",
    "country_data = pd.DataFrame({\n",
    "    'Country': valid_countries.index,\n",
    "    'Count': valid_countries.values\n",
    "})\n",
    "\n",
    "# Länder-Visualisierung\n",
    "country_chart = alt.Chart(country_data).mark_bar().encode(\n",
    "    y=alt.Y('Country:N', \n",
    "            sort='-x',\n",
    "            title='Land'),\n",
    "    x=alt.X('Count:Q',\n",
    "            title='Anzahl Publikationen'),\n",
    "    tooltip=['Country:N', 'Count:Q']\n",
    ").properties(\n",
    "    title='Top 10 Länder in der KI-Software-Engineering-Forschung',\n",
    "    width=1000,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "# Text-Labels für Länder\n",
    "country_text = alt.Chart(country_data).mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=3\n",
    ").encode(\n",
    "    y='Country:N',\n",
    "    x='Count:Q',\n",
    "    text=alt.Text('Count:Q', format='.0f')\n",
    ")\n",
    "\n",
    "final_country_chart = (country_chart + country_text).configure_axis(\n",
    "    grid=True,\n",
    "    gridOpacity=0.3\n",
    ")\n",
    "final_country_chart.save('country_analysis.html')\n",
    "\n",
    "\n",
    "# Statistiken ausgeben\n",
    "print(\"\\nTop 10 Länder und ihre Publikationszahlen:\")\n",
    "for country, count in valid_countries.items():\n",
    "    print(f\"{country}: {int(count)}\")\n",
    "\n",
    "# Statistiken speichern\n",
    "country_data.to_csv('country_statistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Institutionen und ihre Publikationszahlen:\n",
      "State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China: 10\n",
      "Dept. of Computer Science, Iowa State University, Ames, IA, USA: 6\n",
      "David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada: 4\n",
      "Simula Research Laboratory, Lysaker, Norway: 4\n",
      "Singapore Management University, Singapore: 3\n",
      "Monash University, Melbourne, Australia: 3\n",
      "Purdue University, West Lafayette, USA: 3\n",
      "The Chinese University of Hong Kong, Hong Kong, China: 3\n",
      "Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA: 3\n",
      "Department of Informatics, University of Oslo, Oslo, Norway: 3\n",
      "\n",
      "Ländersanalyse abgeschlossen und gespeichert\n"
     ]
    }
   ],
   "source": [
    "# Institutionsanalyse\n",
    "valid_institutions = df['Institution'].dropna().value_counts().head(10)\n",
    "institution_data = pd.DataFrame({\n",
    "    'Institution': valid_institutions.index,\n",
    "    'Count': valid_institutions.values\n",
    "})\n",
    "\n",
    "# Institutions-Visualisierung\n",
    "institution_chart = alt.Chart(institution_data).mark_bar().encode(\n",
    "    y=alt.Y('Institution:N', \n",
    "            sort='-x',\n",
    "            title='Institution'),\n",
    "    x=alt.X('Count:Q',\n",
    "            title='Anzahl Publikationen'),\n",
    "    tooltip=['Institution:N', 'Count:Q']\n",
    ").properties(\n",
    "    title='Top 10 Institutionen in der KI-Software-Engineering-Forschung',\n",
    "    width=1000,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "# Text-Labels für Institutionen\n",
    "institution_text = alt.Chart(institution_data).mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=3\n",
    ").encode(\n",
    "    y='Institution:N',\n",
    "    x='Count:Q',\n",
    "    text=alt.Text('Count:Q', format='.0f')\n",
    ")\n",
    "\n",
    "final_institution_chart = (institution_chart + institution_text).configure_axis(\n",
    "    grid=True,\n",
    "    gridOpacity=0.3\n",
    ")\n",
    "final_institution_chart.save('institution_analysis.html')\n",
    "\n",
    "print(\"\\nTop 10 Institutionen und ihre Publikationszahlen:\")\n",
    "for institution, count in valid_institutions.items():\n",
    "    print(f\"{institution}: {int(count)}\")\n",
    "\n",
    "# Statistiken speichern\n",
    "institution_data.to_csv('institution_statistics.csv', index=False)\n",
    "print(\"\\nLändersanalyse abgeschlossen und gespeichert\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
